@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}



% OUR REFERENCES START HERE
%%%%% LTG UIO %%%%%%%%%
%  NoReC
@InProceedings{
    NRC,
    author = {Erik Velldal and Lilja Øvrelid and Eivind Alexander Bergem and Cathrine Stadsnes and Samia Touileb and Fredrik Jørgensen},
    title = "{NoReC: The Norwegian Review Corpus}",
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
    year = {2018},
    month = may,
    address = {Miyazaki, Japan},
    editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
    publisher = {European Language Resources Association (ELRA)},
    isbn = {979-10-95546-00-9},
    language = {english}
}


% NoReC fine
@inproceedings{
    NRCFINE,
    title = "A Fine-grained Sentiment Dataset for {N}orwegian",
    author = "{\O}vrelid, Lilja  and
      M{\ae}hlum, Petter  and
      Barnes, Jeremy  and
      Velldal, Erik",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.618",
    pages = "5025--5033",
    abstract = "We here introduce NoReC{\_}fine, a dataset for fine-grained sentiment analysis in Norwegian, annotated with respect to polar expressions, targets and holders of opinion. The underlying texts are taken from a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, games, music, products, movies and more. We here present a detailed description of this annotation effort. We provide an overview of the developed annotation guidelines, illustrated with examples and present an analysis of inter-annotator agreement. We also report the first experimental results on the dataset, intended as a preliminary benchmark for further experiments.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}


%  NoReC Eval
@inproceedings{
    NRCEVAL,
    title = "Annotating evaluative sentences for sentiment analysis: a dataset for {N}orwegian",
    author = "M{\ae}hlum, Petter  and
      Barnes, Jeremy  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6113",
    pages = "121--130",
    abstract = "This paper documents the creation of a large-scale dataset of evaluative sentences {--} i.e. both subjective and objective sentences that are found to be sentiment-bearing {--} based on mixed-domain professional reviews from various news-sources. We present both the annotation scheme and first results for classification experiments. The effort represents a step toward creating a Norwegian dataset for fine-grained sentiment analysis.",
}

% Multitask learning for negation
@article{
   MTLNEG,
   title={Improving sentiment analysis with multi-task learning of negation},
   volume={27},
   ISSN={1469-8110},
   url={http://dx.doi.org/10.1017/S1351324920000510},
   DOI={10.1017/s1351324920000510},
   number={2},
   journal={Natural Language Engineering},
   publisher={Cambridge University Press (CUP)},
   author={Barnes, Jeremy and Velldal, Erik and Øvrelid, Lilja},
   year={2020},
   month=nov,
   pages={249–269}
}

% introducing mlt
@inproceedings{
    Caruana1993MultitaskLA,
    title={Multitask Learning: A Knowledge-Based Source of Inductive Bias},
    author={R. Caruana},
    booktitle={ICML},
    year={1993}
}

@misc{
    liu2016recursrent,
    title={Recurrent Neural Network for Text Classification with Multi-Task Learning}, 
    author={Pengfei Liu and Xipeng Qiu and Xuanjing Huang},
    year={2016},
    eprint={1605.05101},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% a version of soft parameter sharing
@inproceedings{
    sogaard-goldberg-2016-deep,
    title = "Deep multi-task learning with low level tasks supervised at lower layers",
    author = "S{\o}gaard, Anders  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2038",
    doi = "10.18653/v1/P16-2038",
    pages = "231--235",
}

% parameter sharing in pytorch blogpost
% https://avivnavon.github.io/blog/parameter-sharing-in-deep-learning/
@misc{
    parametershareblogpost,
    author = {Aviv Navon},
    title = {Parameter Sharing in Deep Learning},
    month = {December},
    year = 2019,
    url = {https://avivnavon.github.io/blog/parameter-sharing-in-deep-learning/},
    urldate = {2021-05-06}
}

@article{
    BiLSTM,
    author = {Schuster, Mike and Paliwal, Kuldip},
    year = {1997},
    month = {12},
    pages = {2673 - 2681},
    title = {Bidirectional recurrent neural networks},
    volume = {45},
    journal = {Signal Processing, IEEE Transactions on},
    doi = {10.1109/78.650093}
}

% BiLSTM CRF
@article{
    HuangXY15,
    author    = {Zhiheng Huang and
               Wei Xu and
               Kai Yu},
    title     = {Bidirectional {LSTM-CRF} Models for Sequence Tagging},
    journal   = {CoRR},
    volume    = {abs/1508.01991},
    year      = {2015},
    url       = {http://arxiv.org/abs/1508.01991},
    archivePrefix = {arXiv},
    eprint    = {1508.01991},
    timestamp = {Tue, 15 Sep 2020 18:57:32 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/HuangXY15.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

% levels benefitting form soft/hard sharing
@misc{
    sanh2018hierarchical,
    title={A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks}, 
    author={Victor Sanh and Thomas Wolf and Sebastian Ruder},
    year={2018},
    eprint={1811.06031},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


% Flaunt it
@article{
  FLAUNT,
  title={If you've got it, flaunt it: Making the most of fine-grained sentiment annotations},
  author={Jeremy Barnes and Lilja {\O}vrelid and Erik Velldal},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.00299}
}

% handwritten
@misc{
    NORBERT,
    title={Large-Scale Contextualised Language Modelling for Norwegian}, 
    author={Andrey Kutuzov and Jeremy Barnes and Erik Velldal and Lilja Øvrelid and Stephan Oepen},
    year={2021},
    eprint={2104.06546},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}




%%%%%  OTHERS   %%%%%%%%%%%%%%%
% Sem. Anal. Opin. Min. book 2012 (handwritten)
@book
{
    SAOM,
    AUTHOR = {Liu, B.},
     TITLE = {Sentiment Analysis and Opinion Mining},
 PUBLISHER = {Morgan &
Claypool Publishers},
      YEAR = {2012},
     PAGES = {168},
}

% Opin. Min Sem Anal book 2008 (handwritten)
@book
{
    OMSA,
    AUTHOR = {Pang, B. and Lee, L.},
     TITLE = {Opinion Mining and Sentiment Analysis},
 PUBLISHER = {Cornell University},
      YEAR = {2008},
     PAGES = {137},
}

% Stanford Treebank (handwritten)
@article 
{
    TREEBANK,
    AUTHOR = {Socher, R. and Perelygin, A. and Wu, J. Y. and Chuang, J. and Manning, C. D. and Ng, A. Y. ands Potts, C.},
     TITLE = {Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank},
 PUBLISHER = {Stanford University},
      YEAR = {2013},
     PAGES = {12},
}


% ABSA SemEval2014 
@inproceedings{
    pontiki-etal-2014-semeval,
    title = "{S}em{E}val-2014 Task 4: Aspect Based Sentiment Analysis",
    author = "Pontiki, Maria  and
      Galanis, Dimitris  and
      Pavlopoulos, John  and
      Papageorgiou, Harris  and
      Androutsopoulos, Ion  and
      Manandhar, Suresh",
    booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S14-2004",
    doi = "10.3115/v1/S14-2004",
    pages = "27--35",
}


% BERT
@inproceedings{
    BERT,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

% bertology
@misc{
    bertology,
    title={A Primer in BERTology: What we know about how BERT works}, 
    author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
    year={2020},
    eprint={2002.12327},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% german bert
@misc{
    gottbert,
    title={GottBERT: a pure German Language Model}, 
    author={Raphael Scheible and Fabian Thomczyk and Patric Tippmann and Victor Jaravine and Martin Boeker},
    year={2020},
    eprint={2012.02110},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% camenbert french bert
@inproceedings{
    camembert,
    title = "{C}amem{BERT}: a Tasty {F}rench Language Model",
    author = "Martin, Louis  and
      Muller, Benjamin  and
      Ortiz Su{\'a}rez, Pedro Javier  and
      Dupont, Yoann  and
      Romary, Laurent  and
      de la Clergerie, {\'E}ric  and
      Seddah, Djam{\'e}  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.645",
    pages = "7203--7219",
    abstract = "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
}

% glove
@misc{
    GloVe,
    title={Linking GloVe with word2vec}, 
    author={Tianze Shi and Zhiyuan Liu},
    year={2014},
    eprint={1411.5595},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% fastText
@misc{
    fastText,
    title={Enriching Word Vectors with Subword Information}, 
    author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
    year={2017},
    eprint={1607.04606},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


% Bidir LSTM Seq. label
@misc{
    huang2015bidirectional,
    title={Bidirectional LSTM-CRF Models for Sequence Tagging}, 
    author={Zhiheng Huang and Wei Xu and Kai Yu},
    year={2015},
    eprint={1508.01991},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% absa bilstm bert 2019
@misc{
    xu2019bert,
    title={BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis}, 
    author={Hu Xu and Bing Liu and Lei Shu and Philip S. Yu},
    year={2019},
    eprint={1904.02232},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


% pyscholinguistic
@article{
    ettinger-2020-bert,
    title = "What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
    author = "Ettinger, Allyson",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.3",
    doi = "10.1162/tacl_a_00298",
    pages = "34--48",
    abstract = "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction{---} and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
}

% GPT 2 OpenAI
@inproceedings{
    Radford2019LanguageMA,
    title={Language Models are Unsupervised Multitask Learners},
    author={A. Radford and Jeffrey Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
    year={2019}
}


% ABSA fine-grained opinion mining
@inproceedings{
    ocampo-diaz-etal-2020-aspect,
    title = "Aspect-Based Sentiment Analysis as Fine-Grained Opinion Mining",
    author = "Ocampo Diaz, Gerardo  and
      Zhang, Xuanming  and
      Ng, Vincent",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.840",
    pages = "6804--6811",
    abstract = "We show how the general fine-grained opinion mining concepts of opinion target and opinion expression are related to aspect-based sentiment analysis (ABSA) and discuss their benefits for resource creation over popular ABSA annotation schemes. Specifically, we first discuss why opinions modeled solely in terms of (entity, aspect) pairs inadequately captures the meaning of the sentiment originally expressed by authors and how opinion expressions and opinion targets can be used to avoid the loss of information. We then design a meaning-preserving annotation scheme and apply it to two popular ABSA datasets, the 2016 SemEval ABSA Restaurant and Laptop datasets. Finally, we discuss the importance of opinion expressions and opinion targets for next-generation ABSA systems. We make our datasets publicly available for download.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

% Reviews english sentence level
@inproceedings{
    toprak-etal-2010-sentence,
    title = "Sentence and Expression Level Annotation of Opinions in User-Generated Discourse",
    author = "Toprak, Cigdem  and
      Jakob, Niklas  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P10-1059",
    pages = "575--584",
}


% eng and german fgsa
@inproceedings{
    klinger-cimiano-2014-usage,
    title = "The {USAGE} review corpus for fine grained multi lingual opinion analysis",
    author = "Klinger, Roman  and
      Cimiano, Philipp",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/85_Paper.pdf",
    pages = "2211--2218",
    abstract = "Opinion mining has received wide attention in recent years. Models for this task are typically trained or evaluated with a manually annotated dataset. However, fine-grained annotation of sentiments including information about aspects and their evaluation is very labour-intensive. The data available so far is limited. Contributing to this situation, this paper describes the Bielefeld University Sentiment Analysis Corpus for German and English (USAGE), which we offer freely to the community and which contains the annotation of product reviews from Amazon with both aspects and subjective phrases. It provides information on segments in the text which denote an aspect or a subjective evaluative phrase which refers to the aspect. Relations and coreferences are explicitly annotated. This dataset contains 622 English and 611 German reviews, allowing to investigate how to port sentiment analysis systems across languages and domains. We describe the methodology how the corpus was created and provide statistics including inter-annotator agreement. We further provide figures for a baseline system and results for German and English as well as in a cross-domain setting. The results are encouraging in that they show that aspects and phrases can be extracted robustly without the need of tuning to a particular type of products.",
}


% movie review adjective intensity
@inproceedings{
    sharma-etal-2015-adjective,
    title = "Adjective Intensity and Sentiment Analysis",
    author = "Sharma, Raksha  and
      Gupta, Mohit  and
      Agarwal, Astha  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1300",
    doi = "10.18653/v1/D15-1300",
    pages = "2520--2526",
}

% older movie corpus
@inproceedings{
    pang-lee-2004-sentimental,
    title = "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts",
    author = "Pang, Bo  and
      Lee, Lillian",
    booktitle = "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04)",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    url = "https://www.aclweb.org/anthology/P04-1035",
    doi = "10.3115/1218955.1218990",
    pages = "271--278",
}


% oldest ABSA I could find
@inproceedings{
    hu_liu_customer_reviews,
    author = {Hu, Minqing and Liu, Bing},
    year = {2004},
    month = {08},
    pages = {168-177},
    title = {Mining and summarizing customer reviews},
    journal = {KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    doi = {10.1145/1014052.1014073}
}


% 2018 bilstm + bicrf
@inproceedings{
    panchendrarajan-amaresan-2018-bidirectional,
    title = "Bidirectional {LSTM}-{CRF} for Named Entity Recognition",
    author = "Panchendrarajan, Rrubaa  and
      Amaresan, Aravindh",
    booktitle = "Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",
    month = dec,
    year = "2018",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/Y18-1061",
}

% negations role in sentiment analysis
@inproceedings{
    wiegand-etal-2010-survey,
    title = "A survey on the role of negation in sentiment analysis",
    author = "Wiegand, Michael  and
      Balahur, Alexandra  and
      Roth, Benjamin  and
      Klakow, Dietrich  and
      Montoyo, Andr{\'e}s",
    booktitle = "Proceedings of the Workshop on Negation and Speculation in Natural Language Processing",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "University of Antwerp",
    url = "https://www.aclweb.org/anthology/W10-3111",
    pages = "60--68",
}